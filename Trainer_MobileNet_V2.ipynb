{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Entrenador (1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JErimKaPujKh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6b17f12-53b6-4483-d4bc-893c81c0625e"
      },
      "source": [
        "pip install -q tflite-model-maker"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 616 kB 7.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 840 kB 62.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 120 kB 68.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 68.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 44.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 54.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 213 kB 44.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 87 kB 5.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 52.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 32.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 43.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 1.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 6.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 47.6 MB 74 kB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 55.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 208 kB 44.6 MB/s \n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwYCNHMFoWZi"
      },
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import save_model\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import AveragePooling2D, Dropout, Flatten, Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import imutils\n",
        "from imutils.video import VideoStream\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-xBroBqorZD"
      },
      "source": [
        "image_path = list(paths.list_images('')) # Se carga el directorio del dataset\n",
        "                  \n",
        "full_data =[] # Vector para recolectar imagenes\n",
        "etiquetas = [] # Vector para recolectar las etiquetas\n",
        "for data_path in image_path:\n",
        "    etiqueta = data_path.split(os.path.sep)[-2] #Se obtiene la etiqueta de cada imagen.\n",
        "    image = img_to_array(load_img(data_path, target_size=(224, 224)))\n",
        "    image = preprocess_input(image) #Convierte la imagen a un formato especifico para Movilnet V2\n",
        "    full_data.append(image) #Agrega la imagen\n",
        "    etiquetas.append(etiqueta) # Agrega la etiqueta\n",
        "data =np.array(full_data,dtype='float32') # Se convierte las imagenes a numpy\n",
        "etiquetas= np.array(etiquetas)  # Se convierte las etiquetas a numpy\n",
        "lb = LabelBinarizer()\n",
        "raw_label = etiquetas\n",
        "etiquetas = lb.fit_transform(etiquetas)\n",
        "etiquetas = to_categorical(etiquetas) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zlaYWsrOCHR"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-YrKe_8qPSU"
      },
      "source": [
        "(x_train, x_test, y_train, y_test) = train_test_split(data, etiquetas, test_size=0.20, stratify=etiquetas, random_state=250)\n",
        "# Se divide la data en 80% de entrenamiento y 20% para testeo."
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyCc0aorq_Um"
      },
      "source": [
        "Main_model = MobileNetV2(weights=\"imagenet\", include_top=False,  # Preperar el MovilNet V2\n",
        "                         input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "Main_out_put = Main_model.output\n",
        "\n",
        "#Creación de la capa de salida\n",
        "head_model = AveragePooling2D(pool_size = (7,7))(Main_out_put)\n",
        "Flatten_layer = Flatten(name=\"flatten\")(head_model)\n",
        "Dense_layer = Dense(128, activation=\"relu\")(Flatten_layer)\n",
        "Dropouts = Dropout(0.5)(Dense_layer)\n",
        "Output_layer = Dense(2, activation=\"softmax\")(Dropouts)\n",
        "\n",
        "\n",
        "model = Model(inputs = Main_model.input, outputs = Output_layer)\n",
        "\n",
        "for layer in Main_model.layers: #Congelar las capas del main model para que no se actualicen en la primera corrida \n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTLJZnGKrWUc"
      },
      "source": [
        "class Optimizer:\n",
        "    def __init__(self, model, mb = 8, lr = 0.0001, loss = tf.keras.losses.binary_crossentropy, \n",
        "               opt=tf.keras.optimizers.Adam, regularization = \"l1\",lamda = 0.01):\n",
        "        self.model     = model \n",
        "        self.loss      = loss \n",
        "        self.optimizer = opt(learning_rate = lr) \n",
        "        self.mb        = mb  \n",
        "        self.l1_l2_regul = self.regularization_type(regularization)\n",
        "        self.reg_const = lamda\n",
        "        self.train_loss     = tf.keras.metrics.Mean()\n",
        "        self.train_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "        self.test_loss     = tf.keras.metrics.Mean()\n",
        "        self.test_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "   \n",
        "    def train_step(self, x , y):\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(x) \n",
        "            loss_temp = self.loss(y, predictions) \n",
        "            loss = self.apply_reg(loss_temp) \n",
        "\n",
        "        #Applying gradients\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables) \n",
        "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "        self.train_loss(loss) \n",
        "        self.train_accuracy(y, predictions) \n",
        "        return loss\n",
        "\n",
        "    \n",
        "    def test_step(self, x , y):\n",
        "        predictions = self.model(x) \n",
        "        loss = self.loss(y, predictions) \n",
        "        self.test_loss(loss) \n",
        "        self.test_accuracy(y, predictions) \n",
        "  \n",
        "    def apply_reg(self,dyn_loss): \n",
        "        reg_loss = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES) \n",
        "        regularizer = self.l1_l2_regul(self.reg_const)(reg_loss) \n",
        "        loss = tf.reduce_mean(dyn_loss + regularizer)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def regularization_type(self,querry):\n",
        "        if querry == \"l1\":\n",
        "            return regularizers.l1\n",
        "        elif querry == \"l2\":\n",
        "            return regularizers.l2\n",
        "\n",
        "\n",
        "    def train (self):\n",
        "        batches =0\n",
        "        for mbX, mbY in self.Augumentation.flow(self.trainX,self.trainY, batch_size = self.mb):\n",
        "            self.train_step(mbX, mbY)\n",
        "            batches +=1\n",
        "            if batches >= len(x_train) / 32:\n",
        "                break\n",
        "  \n",
        "\n",
        "    def test  (self):\n",
        "        batches = 0\n",
        "        for mbX, mbY in self.Augumentation.flow(self.testX,self.testY, batch_size = self.mb):\n",
        "            self.test_step(mbX, mbY)\n",
        "            batches +=1\n",
        "            if batches >= len(x_train) / 32:\n",
        "                break\n",
        "  \n",
        "\n",
        "    def run   (self, dataX, dataY, testX, testY, epochs, verbose=2):\n",
        "        historyTR = [] \n",
        "        historyTS = [] \n",
        "        historyTR_acc = [] \n",
        "        historyTS_acc = [] \n",
        "        template = '{} {}, {}: {}, {}: {}, {}: {},{}: {}' \n",
        "                                                \n",
        "\n",
        "        self.Augumentation = ImageDataGenerator(rotation_range=20,zoom_range=0.15,width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,shear_range=0.15,horizontal_flip=True,fill_mode=\"nearest\")\n",
        "        self.Augumentation.fit(x_train)\n",
        "\n",
        "\n",
        "        self.trainX = dataX\n",
        "        self.trainY = dataY \n",
        "        self.testX  = testX\n",
        "        self.testY = testY \n",
        "\n",
        "        for i in range(epochs):\n",
        "      \n",
        "            self.train () \n",
        "            self.test  () \n",
        "            if verbose > 0:\n",
        "                print(template.format(\"epoch: \", i+1,\" TRAIN LOSS: \", self.train_loss.result(),\n",
        "                              \" TEST LOSS: \" , self.test_loss.result(),\n",
        "                              \" TRAIN ACC: \" , self.train_accuracy.result()*100,\n",
        "                              \" TEST ACC: \"  , self.test_accuracy.result()*100))\n",
        "            temp = '{}'\n",
        "            historyTR.append(float(temp.format(self.train_loss.result())))\n",
        "            historyTS.append(float(temp.format(self.test_loss.result() )))\n",
        "            historyTR_acc.append(float(temp.format(self.train_accuracy.result()*100)))\n",
        "            historyTS_acc.append(float(temp.format(self.test_accuracy.result()*100 )))\n",
        "            self.train_loss.reset_states()\n",
        "            self.train_accuracy.reset_states()\n",
        "            self.test_loss.reset_states()\n",
        "            self.test_accuracy.reset_states()\n",
        "        \n",
        "        return historyTR, historyTS,historyTR_acc,historyTS_acc"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG2hIylIrMoz"
      },
      "source": [
        "opt    = Optimizer (model, mb = 20, lr = 0.0001,regularization = \"l1\",lamda = 0.01 ) \n",
        "epochs=10\n",
        "loss,val_loss,acc,val_acc = opt.run (x_train, y_train, x_test, y_test, epochs, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8fnINAPuLBx"
      },
      "source": [
        "model.save('/content/drive/MyDrive/Mascarillas 4/my_model.h5')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMiXag0du1Aq"
      },
      "source": [
        "epochs_range = range(epochs)\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es7O3_uNvkd8"
      },
      "source": [
        "predIdxs = model.predict(x_test, batch_size=32)\n",
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(y_test.argmax(axis=1), predIdxs,target_names=lb.classes_))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DduRpDzPfOLU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
